import pyarrow as pa
pa.PyExtensionType = pa.ExtensionType
import os, argparse, math, random, json, numpy as np, torch
from typing import Optional, List
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoConfig, AutoModelForCausalLM, AutoModelForMaskedLM,
    Trainer, TrainingArguments, DataCollatorForLanguageModeling
)
from transformers.trainer_utils import get_last_checkpoint
from torch.utils.data import random_split
import pandas as pd

# Optional PEFT
PEFT_AVAILABLE = True
try:
    from peft import LoraConfig, get_peft_model, PeftModel
except Exception:
    PEFT_AVAILABLE = False

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def is_bert_family(name: str) -> bool:
    n = name.lower()
    return any(x in n for x in ["bert-base", "distilbert", "bert-"])

def is_gpt_family(name: str) -> bool:
    n = name.lower()
    return any(x in n for x in ["gpt2", "gpt-2"])

def read_texts_from_dir(data_dir: str) -> List[str]:
    texts = []
    for root, _, files in os.walk(data_dir):
        for f in files:
            if f.endswith(".txt"):
                with open(os.path.join(root, f), "r", encoding="utf-8", errors="ignore") as fh:
                    for line in fh:
                        line = line.strip()
                        if line:
                            texts.append(line)
    if len(texts) == 0:
        raise RuntimeError(f"No .txt files found under {data_dir}")
    return texts

def build_dataset(tokenizer, texts: List[str], block_size: int, is_causal: bool):
    enc = tokenizer(texts, truncation=True, padding=False, return_attention_mask=False)
    ids = [torch.tensor(x, dtype=torch.long) for x in enc["input_ids"] if len(x) > 0]
    all_ids = torch.cat(ids)
    n_blocks = all_ids.size(0) // block_size
    all_ids = all_ids[: n_blocks * block_size]
    all_ids = all_ids.view(n_blocks, block_size)

    def gen():
        for i in range(all_ids.size(0)):
            yield {"input_ids": all_ids[i].tolist()}

    ds = Dataset.from_generator(gen)
    return ds

def split_dataset(ds: Dataset, val_ratio: float = 0.1, seed: int = 42):
    n = len(ds)
    n_val = max(1, int(n * val_ratio))
    n_train = n - n_val
    ds_train, ds_val = random_split(
        ds,
        [n_train, n_val],
        generator=torch.Generator().manual_seed(seed)
    )
    return ds_train, ds_val

def load_schedule(schedule_csv: Optional[str]) -> Optional[pd.DataFrame]:
    if schedule_csv is None:
        return None
    df = pd.read_csv(schedule_csv)
    if "predicted_rank" not in df.columns:
        raise ValueError("schedule CSV must contain 'predicted_rank' column")
    return df

def group_targets_for_peft(model, model_name: str, schedule_df: Optional[pd.DataFrame]):
    all_names = [mn for mn, _ in model.named_modules()]

    def select_names(substrs):
        return [n for n in all_names if any(s in n for s in substrs)]

    if is_gpt_family(model_name):
        cand = select_names(["attn.c_attn", "attn.c_proj", "mlp.c_fc", "mlp.c_proj"])
    else:
        cand = select_names([
            "attention.self.query", "attention.self.key", "attention.self.value",
            "attention.output.dense", "intermediate.dense", "output.dense",
            "attention.q_lin", "attention.k_lin", "attention.v_lin", "attention.out_lin"
        ])
    cand = sorted(set(cand))

    r_bins = [(48, 32), (24, 16), (12, 8), (0, 4)]
    assign = {4: [], 8: [], 16: [], 32: []}

    if schedule_df is not None and "module_name" in schedule_df.columns:
        df = schedule_df.copy()
        df["predicted_rank"] = pd.to_numeric(df["predicted_rank"], errors="coerce").fillna(4).clip(0, 64)
        df = df[df["module_name"].isin(cand)]
        if len(df) > 0:
            for _, row in df.iterrows():
                pr = int(row["predicted_rank"])
                tgt = str(row["module_name"])
                for th, r in r_bins:
                    if pr >= th:
                        assign[r].append(tgt)
                        break
            for r in list(assign.keys()):
                assign[r] = sorted(set(assign[r]))
            return assign

    def layer_index(n: str) -> int:
        import re
        m = re.search(r"\.(\d+)\.", n)
        return int(m.group(1)) if m else 0

    cand_sorted = sorted(cand, key=layer_index)
    L = len(cand_sorted)
    if L == 0:
        return assign
    q1 = int(0.25 * L); q2 = int(0.5 * L); q3 = int(0.75 * L)
    assign[32] = cand_sorted[q3:]
    assign[16] = cand_sorted[q2:q3]
    assign[8]  = cand_sorted[q1:q2]
    assign[4]  = cand_sorted[:q1]
    for r in list(assign.keys()):
        assign[r] = sorted(set(assign[r]))
    return assign

def build_peft_with_schedule(model, model_name: str, schedule_df: Optional[pd.DataFrame], uniform_rank: Optional[int]):
    if not PEFT_AVAILABLE:
        print("PEFT not available; proceeding without LoRA.")
        return model, None

    if uniform_rank and uniform_rank > 0 and schedule_df is None:
        targets = group_targets_for_peft(model, model_name, schedule_df=None)
        all_targets = sorted({n for lst in targets.values() for n in lst})
        if len(all_targets) == 0:
            print("No target modules discovered; proceeding without LoRA.")
            return model, None
        cfg = LoraConfig(
            r=uniform_rank, lora_alpha=uniform_rank * 2, lora_dropout=0.05,
            bias="none", task_type="CAUSAL_LM" if is_gpt_family(model_name) else "SEQ_CLS",
            target_modules=all_targets
        )
        model = get_peft_model(model, cfg)
        model.print_trainable_parameters()
        return model, {"adapters": {"uniform": {"r": uniform_rank, "modules": all_targets}}}

    buckets = group_targets_for_peft(model, model_name, schedule_df=schedule_df)
    first = True
    meta = {"adapters": {}}
    for r, mods in sorted(buckets.items(), key=lambda x: x[0], reverse=True):
        if len(mods) == 0:
            continue
        cfg = LoraConfig(
            r=int(r), lora_alpha=int(r) * 2, lora_dropout=0.05, bias="none",
            task_type="CAUSAL_LM" if is_gpt_family(model_name) else "SEQ_CLS",
            target_modules=mods
        )
        if first:
            model = get_peft_model(model, cfg)
            first = False
            meta["adapters"][f"r{r}"] = {"r": int(r), "modules": mods}
        else:
            try:
                model.add_adapter(f"r{r}", cfg)
                meta["adapters"][f"r{r}"] = {"r": int(r), "modules": mods}
            except Exception:
                pass
    if PEFT_AVAILABLE:
        try:
            from peft import PeftModel
            if isinstance(model, PeftModel):
                try:
                    if hasattr(model, "set_adapter"):
                        model.set_adapter(list(meta["adapters"].keys()))
                except Exception:
                    pass
                model.print_trainable_parameters()
        except Exception:
            pass
    return model, meta

def make_training_args(args):
    common = dict(
        output_dir=args.save_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=max(1, args.batch_size // 2),
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.lr,
        warmup_ratio=0.03,
        weight_decay=0.01,
        logging_steps=50,
        save_strategy="epoch",
        save_total_limit=1,
        load_best_model_at_end=False,
        bf16=not args.fp16 and torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8,
        fp16=args.fp16,
        report_to="none",
        gradient_checkpointing=False,
        optim="adamw_torch",
        logging_dir=os.path.join(args.save_dir, "logs"),
        dataloader_num_workers=0,
        disable_tqdm=False,
    )
    # Handle possible API differences in evaluation_strategy parameter name
    try:
        return TrainingArguments(
            evaluation_strategy="epoch",
            logging_strategy="epoch",
            **common
        )
    except TypeError:
        # Fallback for environments that require eval_strategy
        return TrainingArguments(
            eval_strategy="epoch",
            logging_strategy="epoch",
            **common
        )

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", type=str, required=True, help="HF model id, e.g., gpt2-medium, bert-base-uncased")
    ap.add_argument("--dataset", type=str, required=True, help="Path to folder of .txt files")
    ap.add_argument("--rank_schedule", type=str, default=None, help="CSV from Phase 2 schedules")
    ap.add_argument("--uniform_rank", type=int, default=0, help="Uniform LoRA rank if no schedule is provided")
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--batch_size", type=int, default=16)
    ap.add_argument("--grad_accum", type=int, default=1)
    ap.add_argument("--lr", type=float, default=2e-4)
    ap.add_argument("--block_size", type=int, default=512)
    ap.add_argument("--save_dir", type=str, default="outputs/run")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--fp16", action="store_true")
    args = ap.parse_args()

    set_seed(args.seed)
    os.makedirs(args.save_dir, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token if getattr(tokenizer, "eos_token", None) else tokenizer.cls_token

    if is_gpt_family(args.model):
        _ = AutoConfig.from_pretrained(args.model)
        model = AutoModelForCausalLM.from_pretrained(args.model)
        is_causal = True
    else:
        _ = AutoConfig.from_pretrained(args.model)
        model = AutoModelForMaskedLM.from_pretrained(args.model)
        is_causal = False

    schedule_df = load_schedule(args.rank_schedule) if args.rank_schedule else None
    model, peft_meta = build_peft_with_schedule(model, args.model, schedule_df, args.uniform_rank)

    texts = read_texts_from_dir(args.dataset)
    ds = build_dataset(tokenizer, texts, args.block_size, is_causal=is_causal)
    ds_train, ds_val = split_dataset(ds, val_ratio=0.1, seed=args.seed)

    collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=(not is_causal), mlm_probability=0.15
    )

    train_args = make_training_args(args)

    trainer = Trainer(
        model=model,
        args=train_args,
        train_dataset=ds_train,
        eval_dataset=ds_val,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    ckpt = get_last_checkpoint(args.save_dir) if os.path.isdir(args.save_dir) else None
    trainer.train(resume_from_checkpoint=ckpt)

    # Save final artifacts
    trainer.save_model(args.save_dir)
    tokenizer.save_pretrained(args.save_dir)

    # Also write Trainer state at the run root for convenience
    try:
        trainer.state.save_to_json(os.path.join(args.save_dir, "trainer_state.json"))
    except Exception:
        pass

    # Check if model is a PEFT model
    is_peft_model = False
    if PEFT_AVAILABLE:
        try:
            from peft import PeftModel
            is_peft_model = isinstance(model, PeftModel)
        except Exception:
            pass

    # Save schedule metadata
    with open(os.path.join(args.save_dir, "schedule_meta.json"), "w") as f:
        json.dump({
            "model": args.model,
            "rank_schedule": args.rank_schedule,
            "uniform_rank": args.uniform_rank,
            "peft_used": is_peft_model,
            "peft_meta": peft_meta,
            "block_size": args.block_size,
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "grad_accum": args.grad_accum,
            "lr": args.lr,
        }, f, indent=2)

    # Final evaluation with perplexity for both causal and masked LMs
    eval_metrics = trainer.evaluate()
    loss = float(eval_metrics.get("eval_loss", float("nan")))
    ppl = math.exp(loss) if loss == loss else float("nan")  # exp(loss)
    with open(os.path.join(args.save_dir, "eval_summary.txt"), "w") as f:
        for k, v in eval_metrics.items():
            f.write(f"{k}={v}\n")
        f.write(f"perplexity={ppl}\n")
    print(f"Eval loss: {loss} | Perplexity: {ppl}")

if __name__ == "__main__":
    main()