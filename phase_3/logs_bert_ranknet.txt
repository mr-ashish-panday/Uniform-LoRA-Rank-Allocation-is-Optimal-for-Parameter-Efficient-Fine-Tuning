nohup: ignoring input
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
PEFT not available; proceeding without LoRA.
Traceback (most recent call last):
  File "/home/ashish/paper_5/phase_3/fine_tune.py", line 322, in <module>
    main()
  File "/home/ashish/paper_5/phase_3/fine_tune.py", line 248, in main
    texts = read_texts_from_dir(args.dataset)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ashish/paper_5/phase_3/fine_tune.py", line 43, in read_texts_from_dir
    raise RuntimeError(f"No .txt files found under {data_dir}")
RuntimeError: No .txt files found under /home/ashish/datasets/wiki_text
